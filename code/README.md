# Code for extracting the Empirical ODPs emerging from Wikidata

For extracting the Empirical Ontology Design Patterns (EODPs) from (a portion of) Wikidata, you first need to install KGTK and import a (sub)dump of Wikidata using it.
Indeed, our code relies on KGTK.

Useful links:
- [KGTK documentation](https://kgtk.readthedocs.io/en/latest/)
- [KGTK GitHub repository](https://github.com/usc-isi-i2/kgtk)
- [notebook for importing Wikidata](https://github.com/usc-isi-i2/kgtk-notebooks/blob/main/use-cases/create_wikidata/Wikidata-Useful-Files.ipynb)

After installing KGTK, you need to run the following commands in order to process Wikidata and generate the files you need for the next step.

## Import Wikidata

```
kgtk --debug --timing import-wikidata -i WIKIDATA_JSON_DUMP.json.gz minimal-edge-file CLAIMS.RAW.UNSORTED.tsv.gz --split-en-label-file EN_LABELS.tsv.gz --value-hash-width 6 --claim-id-hash-width 8 --use-kgtkwriter True --use-mgzip-for-input False --use-mgzip-for-output False --use-shm True --procs 6 --mapper-batch-size 5 --max-size-per-mapper-queue 3 --single-mapper-queue True --collect-results True --collect-seperately True --collector-batch-size 5 --collector-queue-per-proc-size 3 --progress-interval 500000 --clean --allow-end-of-day False --repair-month-or-day-zero --minimum-valid-year 1 --maximum-valid-year 9999 --validate-fromisoformat --repair-lax-coordinates --allow-language-suffixes --allow-wikidata-lq-strings
```

```
kgtk --debug --timing filter --verbose --use-mgzip TRUE --input-file CLAIMS.RAW.UNSORTED.tsv.gz --first-match-only --pattern ";; novalue"  -o CLAIMS.NOVALUE.UNSORTED.tsv.gz --pattern ";; somevalue" -o CLAIMS.SOMEVALUE.UNSORTED.tsv.gz --reject-file CLAIMS.UNSORTED.tsv.gz
```

## Generate the subclass of relation file (P279 property)
At this point, you need to generate the file containing all the subclass of (P279) edges present in the claims file.
You can see the see the [notebook for importing Wikidata](https://github.com/usc-isi-i2/kgtk-notebooks/blob/main/use-cases/create_wikidata/Wikidata-Useful-Files.ipynb) for instructions, but we also report here the needed commands you need to run.

## Generate a subgraph from Wikidata
Either you extract patterns from the whole Wikidata, or you extract them from a portion of it.
In the latter case, you first need to generate a .tsv file with the list of Wikidata entities of interest, naming the column "id".
Then, run the following command.

```
kgtk ifexists --input-file CLAIMS.UNSORTED.tsv.gz --filter-on LIST_OF_WD_ENTITIES.tsv --o CLAIMS_SUBGRAPH.tsv.gz
```

## Extract the empirical patterns
Then, you need to run the `wikidata_patterns.sh` bash script from the command line.
You will be asked to give as input:
- the path of the folder where to store all results
- the edgefile of your Wikidata (sub)graph generated by KGTK (either `CLAIMS.UNSORTED.tsv.gz` for the whole Wikidata, or `CLAIMS_SUBGRAPH.tsv.gz` for your Wikidata subgraph)
- the labelfile/nodefile of your Wikidata (sub)graph generated by KGTK (`EN_LABELS.tsv.gz`)
- the edgefile containing the P279star triples from the whole Wikidata dump (`P279STAR.tsv.gz`)
- the edgefile containing the P31 triples from the whole Wikidata dump (we also need the types of the objects of the triples in your subgraph)
- the 3 thresholds to be used for selecting the most common classes, the most common properties per each class, the most common ranges per each property

Refer to the `results/README.md` file to know which files will be generated as a result.
